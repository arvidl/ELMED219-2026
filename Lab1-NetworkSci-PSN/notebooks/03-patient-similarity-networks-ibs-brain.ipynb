{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/arvidl/BMED365-2026/blob/main/labs/Lab1-NetworkSci-PSN/notebooks/03-patient-similarity-networks-ibs-brain.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 2025.01.03 - A. Lundervold\n",
    "\n",
    "Lab 1: using the `elemd219-2025`conda environment \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MMIV-ML/ELMED219-2025/blob/main/Lab1-NetworkSci-PSN/notebooks/03-patient-similarity-networks-ibs-brain.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patient similarity networks - ibs-brain\n",
    "\n",
    "(https://github.com/arvidl/ibs-brain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading and Path Handling\n",
    "\n",
    "- Handles both local and Google Colab environments\n",
    "- Automatically sets up correct paths and downloads data if needed\n",
    "- Ensures code portability across different environments\n",
    "\n",
    "\n",
    "#### Data Preprocessing\n",
    "\n",
    "- Selects numerical columns for analysis\n",
    "- Handles missing values using mean imputation\n",
    "- Standardizes features to ensure equal scale importance\n",
    "- Critical for meaningful similarity calculations\n",
    "\n",
    "\n",
    "#### Network Creation\n",
    "\n",
    "- Uses Euclidean distance to measure patient similarity\n",
    "- Applies Gaussian kernel to convert distances to similarities\n",
    "- Implements k-nearest neighbors approach to ensure balanced connectivity\n",
    "- Uses similarity threshold to maintain meaningful connections\n",
    "- Combines global threshold with local connectivity\n",
    "\n",
    "\n",
    "#### Network Analysis\n",
    "\n",
    "- Calculates basic network metrics (nodes, edges, density)\n",
    "- Computes clustering coefficient to measure patient grouping\n",
    "- Detects communities using modularity optimization\n",
    "- Calculates centrality measures to identify key patients\n",
    "- Provides insights into network structure\n",
    "\n",
    "\n",
    "#### Network Visualization\n",
    "\n",
    "- Uses spring layout for natural clustering visualization\n",
    "- Node sizes reflect degree (number of connections)\n",
    "- Colors indicate centrality (importance in network)\n",
    "- Edge transparency shows similarity strength\n",
    "- Includes colorbar for interpretation\n",
    "- Optimized for readability and insight\n",
    "\n",
    "\n",
    "\n",
    "#### Parameters that can be tuned:\n",
    "\n",
    "- threshold: Controls minimum similarity for connection (0.3 default)\n",
    "- k_nearest: Number of neighbors to connect (8 default)\n",
    "- Node size scaling in visualization\n",
    "- Edge transparency and width\n",
    "- Layout parameters\n",
    "\n",
    "#### The result is a network where:\n",
    "\n",
    "- Similar patients are clustered together\n",
    "- Node sizes show connectivity\n",
    "- Colors show centrality/importance\n",
    "- Communities are visually apparent\n",
    "- Outliers are meaningfully connected\n",
    "- Structure is interpretable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mTypeError: type 'List' is not subscriptable. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# StandardScaler: Standardizes features by removing the mean and scaling to unit variance\n",
    "# Formula: z = (x - \u03bc)/\u03c3\n",
    "# Used to ensure all features contribute equally to the analysis\n",
    "# Normalizing your patient features so that variables with different scales \n",
    "# (e.g., age vs. test scores) can be compared fairly\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# pdist: Computes pairwise distances between observations in n-dimensional space\n",
    "# squareform: Converts distance vector to square distance matrix and vice versa\n",
    "# Example:\n",
    "#   pdist([1,2], [3,4]) -> [distance]\n",
    "#   squareform([distance]) -> [[0, distance],\n",
    "#                             [distance, 0]]\n",
    "# Computing how similar or different patients are from each other, which is essential for \n",
    "# creating the patient similarity network\n",
    "from scipy.spatial.distance import pdist, squareform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mTypeError: type 'List' is not subscriptable. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Step 1: Data Loading and Path Handling with mounting Google Drive\n",
    "def setup_data_path_mounting():\n",
    "    \"\"\"Set up the correct data path whether running locally or in Colab\"\"\"\n",
    "    try:\n",
    "        # Try to import Google Colab specific module\n",
    "        # If this succeeds, we're running in Colab\n",
    "        from google.colab import drive\n",
    "        \n",
    "        # Mount Google Drive to access/save files persistently\n",
    "        drive.mount('/content/drive')\n",
    "        \n",
    "        # Create a data directory in Colab's file system\n",
    "        # -p flag creates parent directories if they don't exist\n",
    "        !mkdir -p /content/data\n",
    "        \n",
    "        # Download the data file from GitHub using wget\n",
    "        # -O flag specifies the output filename and path\n",
    "        !wget -O /content/data/demographics_fs7_rbans_IBS_SSS_imputed_78x48.csv https://raw.githubusercontent.com/arvidl/ibs-brain/main/data/demographics_fs7_rbans_IBS_SSS_imputed_78x48.csv\n",
    "        \n",
    "        # Return Colab's data directory path\n",
    "        return '/content/data'\n",
    "        \n",
    "    except:\n",
    "        # If Google Colab import fails, we're running locally\n",
    "        # Return local path: parent directory + 'data'\n",
    "        return os.path.join(os.path.dirname(os.getcwd()), 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mTypeError: type 'List' is not subscriptable. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Setup data path without mounting Google Drive\n",
    "def setup_data_path():\n",
    "    \"\"\"Set up the correct data path whether running locally or in Colab\"\"\"\n",
    "    try:\n",
    "        # Try to import Colab-specific module to detect environment\n",
    "        from google.colab import drive\n",
    "        \n",
    "        # If we reach this point, we're in Colab, so:\n",
    "        # 1. Create a data directory in Colab's temporary file system\n",
    "        !mkdir -p data\n",
    "        \n",
    "        # 2. Download the data file from GitHub\n",
    "        # -O flag: specify output filename\n",
    "        !wget -O data/demographics_fs7_rbans_IBS_SSS_imputed_78x48.csv https://raw.githubusercontent.com/arvidl/ibs-brain/main/data/demographics_fs7_rbans_IBS_SSS_imputed_78x48.csv\n",
    "        \n",
    "        # 3. Return simple 'data' path for Colab environment\n",
    "        return 'data'\n",
    "        \n",
    "    except:\n",
    "        # If Colab import fails, we're running locally\n",
    "        # Return path that points to data directory one level up from current directory\n",
    "        return os.path.join(os.path.dirname(os.getcwd()), 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mTypeError: type 'List' is not subscriptable. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Step 2: Data Preprocessing\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocess the data for network analysis with NaN handling\"\"\"\n",
    "    \n",
    "    # Substep 1: Select only numerical columns\n",
    "    # - Uses pandas select_dtypes to filter for numerical columns\n",
    "    # - Essential because similarity calculations require numerical data\n",
    "    # - Excludes categorical/text columns that can't be used for similarity\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    # Substep 2: Handle missing values (NaN)\n",
    "    # - Uses mean imputation: replaces NaN with column mean\n",
    "    # - Mean imputation is a simple but effective strategy for missing data\n",
    "    # - Must handle NaN before scaling to avoid errors\n",
    "    df_clean = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
    "    \n",
    "    # Substep 3: Standardize features using StandardScaler\n",
    "    # - Transforms each feature to have mean=0 and std=1\n",
    "    # - Formula: z = (x - \u03bc)/\u03c3\n",
    "    # - Ensures all features contribute equally regardless of original scale\n",
    "    # - Critical when features have different units or ranges\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df_clean)\n",
    "    \n",
    "    # Returns:\n",
    "    # - scaled_data: preprocessed and standardized numerical data\n",
    "    # - numeric_cols: list of numerical column names used\n",
    "    return scaled_data, numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mTypeError: type 'List' is not subscriptable. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Step 3: Network Creation\n",
    "def create_similarity_network(scaled_data, threshold=0.5, k_nearest=5):\n",
    "    \"\"\"\n",
    "    Create a similarity network based on patient features and k-nearest neighbors\n",
    "    where:   Similar patients are connected by edges \n",
    "            Edge weights represent similarity strength\n",
    "            Each patient connects to their k most similar neighbors\n",
    "    Parameters:\n",
    "        scaled_data: Preprocessed patient data (output from preprocess_data)\n",
    "        threshold: Minimum similarity required for connection (default=0.5)\n",
    "        k_nearest: Number of neighbors to connect to each node (default=5)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Substep 1: Calculate pairwise distances between all patients\n",
    "    # - Uses Euclidean distance: sqrt(sum((x1-x2)^2))\n",
    "    # - pdist creates a condensed distance matrix\n",
    "    distances = pdist(scaled_data, metric='euclidean')\n",
    "    \n",
    "    # Substep 2: Convert distances to similarities using Gaussian kernel\n",
    "    # - Formula: exp(-d^2 / (2*sigma^2))\n",
    "    # - sigma is mean distance (adds small epsilon to avoid division by zero)\n",
    "    # - Transforms distances to similarities in range [0,1]\n",
    "    sigma = np.mean(distances) + 1e-8\n",
    "    similarities = np.exp(-distances ** 2 / (2 * sigma ** 2))\n",
    "    # Convert to square matrix format for easier indexing\n",
    "    sim_matrix = squareform(similarities)\n",
    "    \n",
    "    # Substep 3: Create empty network\n",
    "    # - Initialize undirected graph\n",
    "    # - Add nodes for each patient\n",
    "    G = nx.Graph()\n",
    "    n_patients = len(scaled_data)\n",
    "    G.add_nodes_from(range(n_patients))\n",
    "    \n",
    "    # Substep 4: Connect nodes based on similarity\n",
    "    # - For each patient, find k most similar other patients\n",
    "    # - Only add edge if similarity > threshold\n",
    "    # - Combines global threshold with local connectivity\n",
    "    for i in range(n_patients):\n",
    "        # Get indices of k most similar patients (excluding self)\n",
    "        neighbors = np.argsort(sim_matrix[i])[-k_nearest-1:-1]\n",
    "        for j in neighbors:\n",
    "            if sim_matrix[i,j] > threshold:\n",
    "                G.add_edge(i, j, weight=sim_matrix[i,j])\n",
    "    \n",
    "    # Returns:\n",
    "    # - G: NetworkX graph with patients as nodes and similarities as edge weights\n",
    "    # - sim_matrix: Full similarity matrix for potential further analysis\n",
    "    return G, sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Network Analysis\n",
    "def analyze_network(G):\n",
    "    \"\"\"\n",
    "    Calculate and display comprehensive network metrics\n",
    "    The input G is a NetworkX graph object that was created by the create_similarity_network() function.\n",
    "    This function performs three main types of network analysis:\n",
    "    (1) Basic Network Metrics:\n",
    "        - Counts nodes (patients) and edges (connections)\n",
    "        - Calculates average connections per patient\n",
    "        - Measures network density and clustering\n",
    "    (2) Community Detection:\n",
    "        - Identifies groups of closely connected patients\n",
    "        - Uses modularity optimization to find natural groupings\n",
    "    (3) Centrality Analysis:\n",
    "        - Degree centrality shows which patients have most connections\n",
    "        - Betweenness centrality identifies bridge patients between groups\n",
    "    The results help understand the network's structure and identify important patients or patient groups.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Substep 1: Calculate and display basic network statistics\n",
    "    # - Number of nodes: total patients in network\n",
    "    # - Number of edges: total connections between patients\n",
    "    # - Average degree: average number of connections per patient\n",
    "    # - Network density: proportion of possible connections that exist\n",
    "    # - Clustering coefficient: measure of patient grouping tendency\n",
    "    print(\"\\nNetwork Analysis:\")\n",
    "    print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "    print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "    print(f\"Average degree: {2*G.number_of_edges()/G.number_of_nodes():.2f}\")\n",
    "    print(f\"Network density: {nx.density(G):.3f}\")\n",
    "    print(f\"Average clustering coefficient: {nx.average_clustering(G):.3f}\")\n",
    "    \n",
    "    # Substep 2: Detect communities in the network\n",
    "    # - Uses greedy modularity optimization algorithm\n",
    "    # - Groups patients into distinct communities\n",
    "    # - Communities are groups of densely connected patients\n",
    "    communities = list(nx.community.greedy_modularity_communities(G))\n",
    "    print(f\"\\nNumber of communities detected: {len(communities)}\")\n",
    "    \n",
    "    # Substep 3: Calculate centrality measures\n",
    "    # - Degree centrality: how many connections each patient has\n",
    "    # - Betweenness centrality: how important each patient is for connecting others\n",
    "    degree_cent = nx.degree_centrality(G)\n",
    "    betweenness_cent = nx.betweenness_centrality(G)\n",
    "    \n",
    "    # Returns three key network metrics:\n",
    "    # - degree_cent: measure of connection counts\n",
    "    # - betweenness_cent: measure of node importance\n",
    "    # - communities: detected patient groups\n",
    "    return degree_cent, betweenness_cent, communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mTypeError: type 'List' is not subscriptable. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Step 5: Network Visualization\n",
    "def visualize_network(G, node_colors=None, pos=None):\n",
    "    \"\"\"\n",
    "    Enhanced network visualization\n",
    "    This visualization function creates a network plot where:\n",
    "    (1) Node size indicates number of connections\n",
    "    (2) Node color indicates centrality/importance\n",
    "    (3) Edge thickness shows similarity strength\n",
    "    (4) Layout groups similar patients together\n",
    "    (5) Colorbar helps interpret node colors\n",
    "    The result is an intuitive visualization of patient relationships and network structure.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Substep 1: Create figure and axis objects\n",
    "    # - Sets up a large figure size for better visibility\n",
    "    # - Creates explicit axis for proper colorbar placement\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    \n",
    "    # Substep 2: Calculate node positions if not provided\n",
    "    # - Uses spring layout algorithm for natural clustering\n",
    "    # - k parameter controls spacing between nodes\n",
    "    # - More iterations = more stable layout\n",
    "    if pos is None:\n",
    "        pos = nx.spring_layout(G, k=1/np.sqrt(G.number_of_nodes()), iterations=50)\n",
    "    \n",
    "    # Substep 3: Calculate node sizes based on degree\n",
    "    # - Larger nodes = more connections\n",
    "    # - Add 1 to ensure even disconnected nodes are visible\n",
    "    degree_dict = dict(G.degree())\n",
    "    node_sizes = [((v + 1) * 100) for v in degree_dict.values()]\n",
    "    \n",
    "    # Substep 4: Set up node colors\n",
    "    # - Uses provided colors or defaults to degree centrality\n",
    "    # - Higher values = darker color\n",
    "    node_colors = list(node_colors.values()) if node_colors else list(nx.degree_centrality(G).values())\n",
    "    \n",
    "    # Substep 5: Draw nodes\n",
    "    # - Size reflects connectivity\n",
    "    # - Color reflects centrality\n",
    "    # - Uses viridis colormap\n",
    "    # - Partial transparency for better visibility\n",
    "    nx.draw_networkx_nodes(G, pos, \n",
    "                          node_size=node_sizes,\n",
    "                          node_color=node_colors,\n",
    "                          cmap=plt.cm.viridis,\n",
    "                          alpha=0.7,\n",
    "                          ax=ax)\n",
    "    \n",
    "    # Substep 6: Draw edges\n",
    "    # - Width reflects similarity strength\n",
    "    # - Low opacity to reduce visual clutter\n",
    "    edge_weights = [G[u][v].get('weight', 0.1) for u,v in G.edges()]\n",
    "    nx.draw_networkx_edges(G, pos, \n",
    "                          alpha=0.2,\n",
    "                          width=[w*2 for w in edge_weights],\n",
    "                          ax=ax)\n",
    "    \n",
    "    # Substep 7: Add title and remove axis\n",
    "    plt.title('Patient Similarity Network\\n(Node size: degree, Color: centrality)')\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "    # Substep 8: Add colorbar\n",
    "    # - Shows scale for centrality scores\n",
    "    # - Properly aligned with main plot\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis)\n",
    "    sm.set_array([])\n",
    "    plt.colorbar(sm, ax=ax, label='Centrality Score')\n",
    "    \n",
    "    # Substep 9: Adjust layout and display\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_analyze_network(threshold=0.3, k_nearest=8):\n",
    "    \"\"\"\n",
    "    Create and analyze patient similarity network\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    threshold : float, default=0.3\n",
    "        Minimum similarity required for connection\n",
    "        Lower values create more connections\n",
    "    k_nearest : int, default=8\n",
    "        Number of nearest neighbors to connect\n",
    "        Higher values create denser networks\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    G : NetworkX graph\n",
    "        The patient similarity network\n",
    "    metrics : tuple\n",
    "        (degree_centrality, betweenness_centrality, communities)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Setup and load data\n",
    "    # - Sets up correct path for local/Colab environment\n",
    "    # - Loads patient data from CSV file\n",
    "    DATA_PATH = setup_data_path()\n",
    "    DATA_FILE = 'demographics_fs7_rbans_IBS_SSS_imputed_78x48.csv'\n",
    "    df = pd.read_csv(os.path.join(DATA_PATH, DATA_FILE))\n",
    "    print(f\"Loaded data shape: {df.shape}\")\n",
    "\n",
    "    # Step 2: Preprocess data\n",
    "    # - Selects numerical features\n",
    "    # - Handles missing values\n",
    "    # - Standardizes the data\n",
    "    scaled_data, feature_cols = preprocess_data(df)\n",
    "    print(f\"\\nFeatures used: {len(feature_cols)}\")\n",
    "\n",
    "    # Step 3: Create network\n",
    "    # - Uses provided threshold and k_nearest parameters\n",
    "    # - Creates patient similarity network based on feature similarity\n",
    "    G, similarity_matrix = create_similarity_network(scaled_data, \n",
    "                                                   threshold=threshold, \n",
    "                                                   k_nearest=k_nearest)\n",
    "\n",
    "    # Step 4: Analyze network\n",
    "    # - Calculates network metrics\n",
    "    # - Detects communities\n",
    "    # - Computes centrality measures\n",
    "    metrics = analyze_network(G)\n",
    "    \n",
    "    # Step 5: Visualize network\n",
    "    # - Creates network visualization\n",
    "    # - Node size shows connectivity\n",
    "    # - Node color shows centrality\n",
    "    visualize_network(G, metrics[0])  # Using degree_centrality for colors\n",
    "\n",
    "    return G, metrics, scaled_data, feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mTypeError: type 'List' is not subscriptable. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Use default parameters\n",
    "    G, (degree_cent, between_cent, communities), scaled_data, feature_cols = create_and_analyze_network()\n",
    "    \n",
    "    # Or specify custom parameters\n",
    "    # G, metrics = create_and_analyze_network(threshold=0.4, k_nearest=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Patient Similarity Network visualization reveals several interesting patterns:\n",
    "\n",
    "1. **Node Characteristics:**\n",
    "   - Each node represents a patient\n",
    "   - Node size indicates degree (number of connections)\n",
    "   - Colors represent centrality (from purple=low to yellow=high)\n",
    "\n",
    "2. **Network Structure:**\n",
    "   - There are several hub patients (larger nodes) that are well-connected\n",
    "   - A few nodes show high centrality (green/yellow) indicating they are important bridge points\n",
    "   - The network shows clear clustering, suggesting subgroups of similar patients\n",
    "   - Some peripheral patients (smaller, purple nodes) are less connected\n",
    "\n",
    "3. **Clinical Implications:**\n",
    "   - The clusters might represent distinct patient subgroups with similar characteristics\n",
    "   - The bridge nodes (high centrality) could represent patients with characteristics that span multiple groups\n",
    "   - The network is well-connected overall, suggesting gradual transitions between patient types\n",
    "   - Peripheral nodes might represent patients with unique or unusual combinations of characteristics\n",
    "\n",
    "4. **Network Properties:**\n",
    "   - The network appears to have good balance between connectivity and structure\n",
    "   - The chosen threshold and k-nearest neighbor parameters have created meaningful groupings\n",
    "   - The visualization effectively shows both local connections and global structure\n",
    "\n",
    "This visualization could help clinicians identify patient subgroups and understand the relationships between different patient characteristics in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mTypeError: type 'List' is not subscriptable. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Analyze similarity distribution\n",
    "# Step 1: Calculate pairwise distances and convert to similarities\n",
    "distances = pdist(scaled_data, metric='euclidean')\n",
    "# Convert distances to similarities using Gaussian kernel\n",
    "# Formula: exp(-d^2 / mean(d^2))\n",
    "# This transforms distances into similarity scores between 0 and 1\n",
    "# - 1 means identical patients\n",
    "# - 0 means maximally different patients\n",
    "similarities = np.exp(-distances ** 2 / np.mean(distances ** 2))\n",
    "\n",
    "# Step 2: Print basic statistics about similarities\n",
    "print(\"\\nSimilarity statistics:\")\n",
    "print(f\"Min similarity: {similarities.min():.3f}\")  # Most different pair of patients\n",
    "print(f\"Max similarity: {similarities.max():.3f}\")  # Most similar pair of patients\n",
    "print(f\"Mean similarity: {similarities.mean():.3f}\")  # Average similarity\n",
    "print(f\"Median similarity: {np.median(similarities):.3f}\")  # Middle similarity value\n",
    "\n",
    "# Step 3: Visualize the distribution of similarities\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(similarities, bins=50)  # Create histogram with 50 bins\n",
    "plt.title('Distribution of Pairwise Similarities')\n",
    "plt.xlabel('Similarity')  # 0=different, 1=identical\n",
    "plt.ylabel('Count')  # Number of patient pairs with this similarity\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mTypeError: type 'List' is not subscriptable. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Read the data from the CSV file at the ibs-brain repository \n",
    "\n",
    "fn = 'demographics_fs7_rbans_IBS_SSS_imputed_78x48.csv'\n",
    "df = pd.read_csv(f'https://raw.githubusercontent.com/arvidl/ibs-brain/main/data/{fn}')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_With the information in data frame df, modify the steps Step 1, ..., Step 5 such that the similarity is based only on the morphometric variables and the cognitive variables, excluding Subject, Group, IBS_SSS, Age, Gender and Education._\n",
    "\n",
    "_For the visualization in Step 5, mark each node with subject number and gender  (e.g. BGA_046 -> 46/M , ..., BGA_172 -> 172/F) and let nodes having Group == IBS be marked with a square-shaped node and Group == HC be circle-shaped._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mTypeError: type 'List' is not subscriptable. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def create_and_analyze_network(threshold=0.3, k_nearest=8):\n",
    "    \"\"\"\n",
    "    Create and analyze patient similarity network based on morphometric and cognitive variables only\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Setup and load data\n",
    "    DATA_PATH = setup_data_path()\n",
    "    DATA_FILE = 'demographics_fs7_rbans_IBS_SSS_imputed_78x48.csv'\n",
    "    df = pd.read_csv(os.path.join(DATA_PATH, DATA_FILE))\n",
    "    \n",
    "    # Store subject info for later use in visualization\n",
    "    subject_info = df[['Subject', 'Group', 'Gender']].copy()\n",
    "    \n",
    "    # Step 2: Preprocess data - select only morphometric and cognitive variables\n",
    "    # Exclude demographic and clinical variables\n",
    "    exclude_cols = ['Subject', 'Group', 'IBS_SSS', 'Age', 'Gender', 'Education']\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    # Preprocess selected features\n",
    "    scaled_data = StandardScaler().fit_transform(df[feature_cols])\n",
    "    print(f\"Features used: {len(feature_cols)}\")\n",
    "\n",
    "    # Step 3: Create network\n",
    "    G, similarity_matrix = create_similarity_network(scaled_data, \n",
    "                                                   threshold=threshold, \n",
    "                                                   k_nearest=k_nearest)\n",
    "\n",
    "    # Step 4: Analyze network\n",
    "    metrics = analyze_network(G)\n",
    "    \n",
    "    # Step 5: Enhanced visualization with subject labels and group-based node shapes\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    \n",
    "    # Calculate layout\n",
    "    pos = nx.spring_layout(G, k=1/np.sqrt(G.number_of_nodes()), iterations=50)\n",
    "    \n",
    "    # Prepare node colors and sizes\n",
    "    node_colors = list(metrics[0].values())\n",
    "    degree_dict = dict(G.degree())\n",
    "    node_sizes = [((v + 1) * 100) for v in degree_dict.values()]\n",
    "    \n",
    "    # Draw nodes based on group (IBS=square, HC=circle)\n",
    "    ibs_nodes = [i for i, row in subject_info.iterrows() if row['Group'] == 'IBS']\n",
    "    hc_nodes = [i for i, row in subject_info.iterrows() if row['Group'] == 'HC']\n",
    "    \n",
    "    # Draw IBS nodes (squares)\n",
    "    nx.draw_networkx_nodes(G, pos, \n",
    "                          nodelist=ibs_nodes,\n",
    "                          node_color=[node_colors[i] for i in ibs_nodes],\n",
    "                          node_size=[node_sizes[i] for i in ibs_nodes],\n",
    "                          node_shape='s',  # square\n",
    "                          cmap=plt.cm.viridis,\n",
    "                          alpha=0.7,\n",
    "                          ax=ax)\n",
    "    \n",
    "    # Draw HC nodes (circles)\n",
    "    nx.draw_networkx_nodes(G, pos, \n",
    "                          nodelist=hc_nodes,\n",
    "                          node_color=[node_colors[i] for i in hc_nodes],\n",
    "                          node_size=[node_sizes[i] for i in hc_nodes],\n",
    "                          node_shape='o',  # circle\n",
    "                          cmap=plt.cm.viridis,\n",
    "                          alpha=0.7,\n",
    "                          ax=ax)\n",
    "    \n",
    "    # Draw edges\n",
    "    edge_weights = [G[u][v].get('weight', 0.1) for u,v in G.edges()]\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.2, width=[w*2 for w in edge_weights])\n",
    "    \n",
    "    # Add node labels (subject number and gender)\n",
    "    labels = {i: f\"{row['Subject'].split('_')[1]}/{row['Gender']}\" \n",
    "             for i, row in subject_info.iterrows()}\n",
    "    nx.draw_networkx_labels(G, pos, labels, font_size=8)\n",
    "    \n",
    "    plt.title('Patient Similarity Network\\n(Square=IBS, Circle=HC)\\n(Node size: degree, Color: centrality)')\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "    # Add colorbar\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis)\n",
    "    sm.set_array([])\n",
    "    plt.colorbar(sm, ax=ax, label='Centrality Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return G, metrics, scaled_data, feature_cols\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    G, metrics, scaled_data, feature_cols = create_and_analyze_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Display the graph for each community, separately._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mTypeError: type 'List' is not subscriptable. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def visualize_communities(G, communities, subject_info, metrics):\n",
    "    \"\"\"\n",
    "    Display separate graphs for each community in the network\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    G : NetworkX graph\n",
    "        The patient similarity network\n",
    "    communities : list\n",
    "        List of sets containing node indices for each community\n",
    "    subject_info : DataFrame\n",
    "        Contains Subject, Group, and Gender information\n",
    "    metrics : tuple\n",
    "        (degree_centrality, betweenness_centrality, communities)\n",
    "    \"\"\"\n",
    "    \n",
    "    # For each community\n",
    "    for idx, community in enumerate(communities):\n",
    "        # Create subgraph for this community\n",
    "        subG = G.subgraph(community)\n",
    "        \n",
    "        # Create new figure for this community\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        \n",
    "        # Calculate layout for this subgraph\n",
    "        pos = nx.spring_layout(subG, k=1/np.sqrt(len(community)), iterations=50)\n",
    "        \n",
    "        # Prepare node colors and sizes for this community\n",
    "        node_colors = [metrics[0][node] for node in subG.nodes()]\n",
    "        node_sizes = [((subG.degree(node) + 1) * 100) for node in subG.nodes()]\n",
    "        \n",
    "        # Separate IBS and HC nodes\n",
    "        ibs_nodes = [i for i in subG.nodes() if subject_info.iloc[i]['Group'] == 'IBS']\n",
    "        hc_nodes = [i for i in subG.nodes() if subject_info.iloc[i]['Group'] == 'HC']\n",
    "        \n",
    "        # Draw IBS nodes (squares)\n",
    "        if ibs_nodes:\n",
    "            nx.draw_networkx_nodes(subG, pos,\n",
    "                                 nodelist=ibs_nodes,\n",
    "                                 node_color=[metrics[0][i] for i in ibs_nodes],\n",
    "                                 node_size=[node_sizes[list(subG.nodes()).index(i)] for i in ibs_nodes],\n",
    "                                 node_shape='s',\n",
    "                                 cmap=plt.cm.viridis,\n",
    "                                 alpha=0.7,\n",
    "                                 ax=ax)\n",
    "        \n",
    "        # Draw HC nodes (circles)\n",
    "        if hc_nodes:\n",
    "            nx.draw_networkx_nodes(subG, pos,\n",
    "                                 nodelist=hc_nodes,\n",
    "                                 node_color=[metrics[0][i] for i in hc_nodes],\n",
    "                                 node_size=[node_sizes[list(subG.nodes()).index(i)] for i in hc_nodes],\n",
    "                                 node_shape='o',\n",
    "                                 cmap=plt.cm.viridis,\n",
    "                                 alpha=0.7,\n",
    "                                 ax=ax)\n",
    "        \n",
    "        # Draw edges\n",
    "        edge_weights = [subG[u][v].get('weight', 0.1) for u,v in subG.edges()]\n",
    "        nx.draw_networkx_edges(subG, pos, alpha=0.2, width=[w*2 for w in edge_weights])\n",
    "        \n",
    "        # Add node labels\n",
    "        labels = {i: f\"{subject_info.iloc[i]['Subject'].split('_')[1]}/{subject_info.iloc[i]['Gender']}\"\n",
    "                 for i in subG.nodes()}\n",
    "        nx.draw_networkx_labels(subG, pos, labels, font_size=8)\n",
    "        \n",
    "        # Add title and stats for this community\n",
    "        plt.title(f'Community {idx+1}\\n'\n",
    "                 f'Size: {len(community)} patients\\n'\n",
    "                 f'IBS: {len(ibs_nodes)}, HC: {len(hc_nodes)}')\n",
    "        ax.set_axis_off()\n",
    "        \n",
    "        # Add colorbar\n",
    "        sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis)\n",
    "        sm.set_array([])\n",
    "        plt.colorbar(sm, ax=ax, label='Centrality Score')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print community statistics\n",
    "        print(f\"\\nCommunity {idx+1} Statistics:\")\n",
    "        print(f\"Total patients: {len(community)}\")\n",
    "        print(f\"IBS patients: {len(ibs_nodes)} ({len(ibs_nodes)/len(community)*100:.1f}%)\")\n",
    "        print(f\"HC patients: {len(hc_nodes)} ({len(hc_nodes)/len(community)*100:.1f}%)\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Modify the main function to include community visualization\n",
    "if __name__ == \"__main__\":\n",
    "    G, (degree_cent, between_cent, communities), scaled_data, feature_cols = create_and_analyze_network()\n",
    "    \n",
    "    # Get subject info from original data\n",
    "    DATA_PATH = setup_data_path()\n",
    "    DATA_FILE = 'demographics_fs7_rbans_IBS_SSS_imputed_78x48.csv'\n",
    "    df = pd.read_csv(os.path.join(DATA_PATH, DATA_FILE))\n",
    "    subject_info = df[['Subject', 'Group', 'Gender']]\n",
    "    \n",
    "    # Visualize each community\n",
    "    visualize_communities(G, communities, subject_info, (degree_cent, between_cent, communities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Please add the statistics (or distribution or values) of the variables: Subject, Group, IBS_SSS, Age, Gender and Education for each of the generated communities._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mTypeError: type 'List' is not subscriptable. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def analyze_community_demographics(community, df):\n",
    "    \"\"\"\n",
    "    Analyze demographic and clinical variables for a community\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    community : set\n",
    "        Set of node indices for the community\n",
    "    df : DataFrame\n",
    "        Original dataframe with all patient information\n",
    "    \"\"\"\n",
    "    # Get data for this community\n",
    "    community_df = df.iloc[list(community)]\n",
    "    \n",
    "    print(\"\\nDetailed Community Statistics:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Group distribution\n",
    "    group_dist = community_df['Group'].value_counts()\n",
    "    print(\"\\nGroup Distribution:\")\n",
    "    for group, count in group_dist.items():\n",
    "        print(f\"{group}: {count} ({count/len(community_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Gender distribution\n",
    "    gender_dist = community_df['Gender'].value_counts()\n",
    "    print(\"\\nGender Distribution:\")\n",
    "    for gender, count in gender_dist.items():\n",
    "        print(f\"{gender}: {count} ({count/len(community_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Numerical variables statistics\n",
    "    print(\"\\nNumerical Variables Statistics:\")\n",
    "    for var in ['Age', 'Education', 'IBS_SSS']:\n",
    "        data = community_df[var].dropna()\n",
    "        if len(data) > 0:\n",
    "            print(f\"\\n{var}:\")\n",
    "            print(f\"Mean \u00b1 SD: {data.mean():.1f} \u00b1 {data.std():.1f}\")\n",
    "            print(f\"Median [IQR]: {data.median():.1f} [{data.quantile(0.25):.1f}-{data.quantile(0.75):.1f}]\")\n",
    "            print(f\"Range: {data.min():.1f}-{data.max():.1f}\")\n",
    "    \n",
    "    # Optional: Create visualizations for numerical variables\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Age distribution\n",
    "    sns.boxplot(data=community_df, y='Age', x='Group', ax=axes[0])\n",
    "    axes[0].set_title('Age Distribution by Group')\n",
    "    \n",
    "    # Education distribution\n",
    "    sns.boxplot(data=community_df, y='Education', x='Group', ax=axes[1])\n",
    "    axes[1].set_title('Education Distribution by Group')\n",
    "    \n",
    "    # IBS_SSS distribution (only for IBS patients)\n",
    "    ibs_data = community_df[community_df['Group'] == 'IBS']\n",
    "    if len(ibs_data) > 0:\n",
    "        sns.boxplot(data=ibs_data, y='IBS_SSS', ax=axes[2])\n",
    "        axes[2].set_title('IBS Severity Score\\n(IBS patients only)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_communities(G, communities, subject_info, metrics, df):\n",
    "    \"\"\"\n",
    "    Display separate graphs for each community with detailed demographics\n",
    "    \"\"\"\n",
    "    for idx, community in enumerate(communities):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Community {idx+1} Analysis\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Create subgraph for this community\n",
    "        subG = G.subgraph(community)\n",
    "        \n",
    "        # Create figure for network visualization\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        \n",
    "        # Calculate layout for this subgraph\n",
    "        pos = nx.spring_layout(subG, k=1/np.sqrt(len(community)), iterations=50)\n",
    "        \n",
    "        # Separate IBS and HC nodes\n",
    "        ibs_nodes = [i for i in community if df.iloc[i]['Group'] == 'IBS']\n",
    "        hc_nodes = [i for i in community if df.iloc[i]['Group'] == 'HC']\n",
    "        \n",
    "        # Draw IBS nodes (squares)\n",
    "        if ibs_nodes:\n",
    "            nx.draw_networkx_nodes(subG, pos,\n",
    "                                 nodelist=ibs_nodes,\n",
    "                                 node_color=[metrics[0][i] for i in ibs_nodes],\n",
    "                                 node_size=500,\n",
    "                                 node_shape='s',\n",
    "                                 cmap=plt.cm.viridis,\n",
    "                                 alpha=0.7,\n",
    "                                 ax=ax)\n",
    "        \n",
    "        # Draw HC nodes (circles)\n",
    "        if hc_nodes:\n",
    "            nx.draw_networkx_nodes(subG, pos,\n",
    "                                 nodelist=hc_nodes,\n",
    "                                 node_color=[metrics[0][i] for i in hc_nodes],\n",
    "                                 node_size=500,\n",
    "                                 node_shape='o',\n",
    "                                 cmap=plt.cm.viridis,\n",
    "                                 alpha=0.7,\n",
    "                                 ax=ax)\n",
    "        \n",
    "        # Draw edges\n",
    "        nx.draw_networkx_edges(subG, pos, \n",
    "                             alpha=0.2,\n",
    "                             width=1,\n",
    "                             ax=ax)\n",
    "        \n",
    "        # Add node labels\n",
    "        labels = {i: f\"{df.iloc[i]['Subject'].split('_')[1]}/{df.iloc[i]['Gender']}\"\n",
    "                 for i in community}\n",
    "        nx.draw_networkx_labels(subG, pos, labels, font_size=8)\n",
    "        \n",
    "        plt.title(f'Community {idx+1}\\n'\n",
    "                 f'Size: {len(community)} patients\\n'\n",
    "                 f'IBS: {len(ibs_nodes)}, HC: {len(hc_nodes)}')\n",
    "        ax.set_axis_off()\n",
    "        \n",
    "        # Add colorbar\n",
    "        sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis)\n",
    "        sm.set_array([])\n",
    "        plt.colorbar(sm, ax=ax, label='Centrality Score')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Add demographic analysis\n",
    "        analyze_community_demographics(community, df)\n",
    "\n",
    "# Modified main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load full dataset\n",
    "    DATA_PATH = setup_data_path()\n",
    "    DATA_FILE = 'demographics_fs7_rbans_IBS_SSS_imputed_78x48.csv'\n",
    "    df = pd.read_csv(os.path.join(DATA_PATH, DATA_FILE))\n",
    "    \n",
    "    # Create and analyze network\n",
    "    G, (degree_cent, between_cent, communities), scaled_data, feature_cols = create_and_analyze_network()\n",
    "    \n",
    "    # Visualize communities with demographic analysis\n",
    "    visualize_communities(G, communities, df[['Subject', 'Group', 'Gender']], \n",
    "                         (degree_cent, between_cent, communities), df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Based on this PSN methodology, can you suggest a way to classify or characterize a new subject, say BGA_900, given the values for alle the variables in the data frame df?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mTypeError: type 'List' is not subscriptable. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Add required import\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def characterize_new_subject(new_subject_data, df, G, communities, k_nearest=5):\n",
    "    \"\"\"\n",
    "    Characterize a new subject based on existing PSN\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    new_subject_data : pd.Series\n",
    "        Data for new subject with same variables as df\n",
    "    df : pd.DataFrame\n",
    "        Original dataset used to create PSN\n",
    "    G : networkx.Graph\n",
    "        Existing patient similarity network\n",
    "    communities : list\n",
    "        List of communities from original network\n",
    "    k_nearest : int\n",
    "        Number of most similar patients to consider\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Results of characterization\n",
    "    \"\"\"\n",
    "    # Step 1: Preprocess new subject data\n",
    "    # Use same feature selection and scaling as original network\n",
    "    exclude_cols = ['Subject', 'Group', 'IBS_SSS', 'Age', 'Gender', 'Education']\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    # Get original data for scaling\n",
    "    X_orig = df[feature_cols].values\n",
    "    \n",
    "    # Combine new subject with original data for proper scaling\n",
    "    new_data = new_subject_data[feature_cols].values.reshape(1, -1)\n",
    "    X_combined = np.vstack([X_orig, new_data])\n",
    "    \n",
    "    # Scale all data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_combined)\n",
    "    \n",
    "    # Extract scaled new subject data\n",
    "    new_scaled = X_scaled[-1:]\n",
    "    \n",
    "    # Step 2: Calculate similarities to all existing patients\n",
    "    distances = cdist(new_scaled, X_scaled[:-1], metric='euclidean')\n",
    "    sigma = np.mean(distances) + 1e-8\n",
    "    similarities = np.exp(-distances ** 2 / (2 * sigma ** 2))\n",
    "    \n",
    "    # Step 3: Find k most similar patients\n",
    "    k_nearest_idx = np.argsort(similarities[0])[-k_nearest:]\n",
    "    k_nearest_similarities = similarities[0][k_nearest_idx]\n",
    "    \n",
    "    # Step 4: Analyze community membership of similar patients\n",
    "    community_votes = []\n",
    "    for idx in k_nearest_idx:\n",
    "        for comm_idx, comm in enumerate(communities):\n",
    "            if idx in comm:\n",
    "                community_votes.append(comm_idx)\n",
    "                break\n",
    "    \n",
    "    # Get most common community\n",
    "    from collections import Counter\n",
    "    predicted_community = Counter(community_votes).most_common(1)[0][0]\n",
    "    \n",
    "    # Step 5: Analyze characteristics of similar patients\n",
    "    similar_patients = df.iloc[k_nearest_idx]\n",
    "    \n",
    "    # Calculate predicted characteristics based on weighted average\n",
    "    weighted_predictions = {}\n",
    "    for col in ['Age', 'Education', 'IBS_SSS']:\n",
    "        if col in similar_patients.columns:\n",
    "            weights = k_nearest_similarities / np.sum(k_nearest_similarities)\n",
    "            weighted_predictions[col] = np.average(\n",
    "                similar_patients[col], weights=weights\n",
    "            )\n",
    "    \n",
    "    # Predict group based on majority voting with similarity weights\n",
    "    group_votes = similar_patients['Group'].value_counts()\n",
    "    predicted_group = group_votes.index[0]\n",
    "    group_confidence = group_votes.iloc[0] / len(k_nearest_idx)\n",
    "    \n",
    "    # Prepare results\n",
    "    results = {\n",
    "        'most_similar_patients': list(zip(\n",
    "            similar_patients['Subject'], \n",
    "            k_nearest_similarities\n",
    "        )),\n",
    "        'predicted_community': predicted_community,\n",
    "        'predicted_group': predicted_group,\n",
    "        'group_confidence': group_confidence,\n",
    "        'predicted_values': weighted_predictions,\n",
    "        'community_distribution': Counter(community_votes)\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nCharacterization of new subject:\")\n",
    "    print(f\"Predicted community: {predicted_community}\")\n",
    "    print(f\"Predicted group: {predicted_group} (confidence: {group_confidence:.2f})\")\n",
    "    print(\"\\nMost similar patients:\")\n",
    "    for subject, sim in results['most_similar_patients']:\n",
    "        print(f\"  {subject}: similarity = {sim:.3f}\")\n",
    "    print(\"\\nPredicted values:\")\n",
    "    for var, val in weighted_predictions.items():\n",
    "        print(f\"  {var}: {val:.2f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a hypothetical new subject\n",
    "    new_subject = df.iloc[0].copy()  # Copy an existing patient as example\n",
    "    new_subject['Subject'] = 'BGA_900'  # Change ID\n",
    "    \n",
    "    # Characterize the new subject\n",
    "    results = characterize_new_subject(\n",
    "        new_subject, \n",
    "        df, \n",
    "        G, \n",
    "        communities\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach:\n",
    "\n",
    "1. **Preprocesses** the new subject's data using the same method as the original network\n",
    "\n",
    "2. **Finds Similar Patients** by:\n",
    "   - Calculating similarities to all existing patients\n",
    "   - Identifying k most similar patients\n",
    "   - Analyzing their characteristics\n",
    "\n",
    "3. **Makes Predictions** about:\n",
    "   - Community membership (which subgroup)\n",
    "   - Patient group (IBS vs HC)\n",
    "   - Clinical/demographic values (weighted by similarity)\n",
    "\n",
    "4. **Provides Confidence Measures** through:\n",
    "   - Similarity scores\n",
    "   - Voting distributions\n",
    "   - Weighted predictions\n",
    "\n",
    "5. **Returns Detailed Results** including:\n",
    "   - Most similar existing patients\n",
    "   - Predicted characteristics\n",
    "   - Confidence measures\n",
    "   - Community assignment\n",
    "\n",
    "This method allows you to:\n",
    "- Place new patients in the existing network structure\n",
    "- Predict their likely characteristics\n",
    "- Understand their relationship to existing patient groups\n",
    "- Get confidence measures for predictions\n",
    "\n",
    "The predictions are based on the characteristics of similar existing patients, weighted by their similarity to the new patient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Based on this approach, can you suggest a leave-one-out validation technique for predicting new cases based on PSN methodology?_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mTypeError: type 'List' is not subscriptable. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def leave_one_out_validation(df, threshold=0.3, k_nearest=8, k_similar=5):\n",
    "    \"\"\"\n",
    "    Perform leave-one-out validation using PSN methodology\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Original dataset\n",
    "    threshold : float\n",
    "        Similarity threshold for network creation\n",
    "    k_nearest : int\n",
    "        Number of neighbors for network creation\n",
    "    k_similar : int\n",
    "        Number of similar patients for prediction\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Validation results\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "    \n",
    "    # Store predictions and actual values\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "    community_predictions = []\n",
    "    similarities_list = []\n",
    "    \n",
    "    # For each patient in the dataset\n",
    "    for i in range(len(df)):\n",
    "        print(f\"\\nValidating patient {i+1}/{len(df)}\")\n",
    "        \n",
    "        # Step 1: Split data into test (current patient) and training (rest)\n",
    "        test_patient = df.iloc[i]\n",
    "        train_df = df.drop(index=df.index[i])\n",
    "        \n",
    "        # Step 2: Create network without current patient\n",
    "        # Preprocess training data\n",
    "        exclude_cols = ['Subject', 'Group', 'IBS_SSS', 'Age', 'Gender', 'Education']\n",
    "        feature_cols = [col for col in train_df.columns if col not in exclude_cols]\n",
    "        scaled_data = StandardScaler().fit_transform(train_df[feature_cols])\n",
    "        \n",
    "        # Create network\n",
    "        G, _ = create_similarity_network(scaled_data, threshold=threshold, k_nearest=k_nearest)\n",
    "        \n",
    "        # Analyze network\n",
    "        _, _, communities = analyze_network(G)\n",
    "        \n",
    "        # Step 3: Predict current patient\n",
    "        results = characterize_new_subject(\n",
    "            test_patient,\n",
    "            train_df,\n",
    "            G,\n",
    "            communities,\n",
    "            k_nearest=k_similar\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        predictions.append(results['predicted_group'])\n",
    "        true_values.append(test_patient['Group'])\n",
    "        community_predictions.append(results['predicted_community'])\n",
    "        similarities_list.append(results['most_similar_patients'])\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(true_values, predictions)\n",
    "    conf_matrix = confusion_matrix(true_values, predictions)\n",
    "    \n",
    "    # Analyze community stability\n",
    "    community_analysis = pd.DataFrame({\n",
    "        'True_Group': true_values,\n",
    "        'Predicted_Group': predictions,\n",
    "        'Predicted_Community': community_predictions\n",
    "    })\n",
    "    \n",
    "    # Calculate average similarity scores\n",
    "    avg_similarities = np.mean([\n",
    "        [sim for _, sim in patient_sims] \n",
    "        for patient_sims in similarities_list\n",
    "    ])\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nValidation Results:\")\n",
    "    print(f\"Overall Accuracy: {accuracy:.3f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(f\"\\nAverage Similarity Score: {avg_similarities:.3f}\")\n",
    "    \n",
    "    # Analyze predictions by group\n",
    "    group_accuracy = pd.DataFrame({\n",
    "        'True': true_values,\n",
    "        'Predicted': predictions\n",
    "    }).groupby('True').apply(\n",
    "        lambda x: accuracy_score(x['True'], x['Predicted'])\n",
    "    )\n",
    "    print(\"\\nAccuracy by Group:\")\n",
    "    print(group_accuracy)\n",
    "    \n",
    "    # Optional: Analyze misclassifications\n",
    "    misclassified = pd.DataFrame({\n",
    "        'Subject': df['Subject'],\n",
    "        'True_Group': true_values,\n",
    "        'Predicted_Group': predictions,\n",
    "        'Community': community_predictions\n",
    "    })\n",
    "    misclassified['Correct'] = misclassified['True_Group'] == misclassified['Predicted_Group']\n",
    "    \n",
    "    # Return detailed results\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'predictions': misclassified,\n",
    "        'avg_similarities': avg_similarities,\n",
    "        'group_accuracy': group_accuracy,\n",
    "        'community_analysis': community_analysis\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Run leave-one-out validation\n",
    "    validation_results = leave_one_out_validation(\n",
    "        df,\n",
    "        threshold=0.3,\n",
    "        k_nearest=8,\n",
    "        k_similar=5\n",
    "    )\n",
    "    \n",
    "    # Additional analysis of results\n",
    "    print(\"\\nDetailed Analysis of Misclassifications:\")\n",
    "    misclassified_cases = validation_results['predictions'][\n",
    "        ~validation_results['predictions']['Correct']\n",
    "    ]\n",
    "    print(misclassified_cases)\n",
    "    \n",
    "    # Visualize results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(validation_results['confusion_matrix'], \n",
    "                annot=True, \n",
    "                fmt='d',\n",
    "                xticklabels=['HC', 'IBS'],\n",
    "                yticklabels=['HC', 'IBS'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This validation approach:\n",
    "\n",
    "1. **For Each Patient:**\n",
    "   - Removes them from the dataset\n",
    "   - Creates network without them\n",
    "   - Predicts their characteristics using remaining patients\n",
    "   - Compares predictions to actual values\n",
    "\n",
    "2. **Calculates Performance Metrics:**\n",
    "   - Overall accuracy\n",
    "   - Group-specific accuracy\n",
    "   - Confusion matrix\n",
    "   - Average similarity scores\n",
    "\n",
    "3. **Analyzes Predictions:**\n",
    "   - Community stability\n",
    "   - Misclassification patterns\n",
    "   - Similarity distributions\n",
    "\n",
    "4. **Provides Insights About:**\n",
    "   - Model reliability\n",
    "   - Group-specific performance\n",
    "   - Network stability\n",
    "   - Prediction confidence\n",
    "\n",
    "This helps:\n",
    "- Validate the PSN methodology\n",
    "- Identify potential biases\n",
    "- Understand prediction reliability\n",
    "- Find areas for improvement\n",
    "\n",
    "The results can guide:\n",
    "- Parameter tuning (threshold, k_nearest)\n",
    "- Feature selection\n",
    "- Methodology improvements\n",
    "- Clinical applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Please explain Step 3 (Network Creation),  Step 4 (Network Analysis) and community detection mathematically - use LaTeX_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the mathematical explanation of the network creation, analysis, and community detection steps:\n",
    "\n",
    "**Step 3: Network Creation**\n",
    "\n",
    "1. **Distance Calculation**:\n",
    "For patients $i$ and $j$ with feature vectors $x_i, x_j \\in \\mathbb{R}^d$, the Euclidean distance is:\n",
    "$$\n",
    "d_{ij} = \\|x_i - x_j\\| = \\sqrt{\\sum_{k=1}^d (x_{ik} - x_{jk})^2}\n",
    "$$\n",
    "\n",
    "2. **Similarity Transformation**:\n",
    "Convert distances to similarities using Gaussian kernel:\n",
    "$$\n",
    "s_{ij} = \\exp\\left(-\\frac{d_{ij}^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "where $\\sigma$ is the mean distance: $\\sigma = \\frac{1}{n(n-1)}\\sum_{i,j} d_{ij}$\n",
    "\n",
    "3. **Network Construction**:\n",
    "For each patient $i$:\n",
    "- Find $k$ nearest neighbors: $\\mathcal{N}_k(i) = \\{j : s_{ij} \\text{ is among } k \\text{ largest similarities}\\}$\n",
    "- Add edge $(i,j)$ if:\n",
    "$$\n",
    "j \\in \\mathcal{N}_k(i) \\text{ and } s_{ij} > \\text{threshold}\n",
    "$$\n",
    "\n",
    "**Step 4: Network Analysis**\n",
    "\n",
    "1. **Basic Metrics**:\n",
    "- Network density: $\\delta = \\frac{2|E|}{|V|(|V|-1)}$\n",
    "- Average degree: $\\bar{k} = \\frac{2|E|}{|V|}$\n",
    "- Clustering coefficient for node $i$:\n",
    "$$\n",
    "C_i = \\frac{2|\\{(j,k) \\in E : j,k \\in \\mathcal{N}(i)\\}|}{k_i(k_i-1)}\n",
    "$$\n",
    "where $k_i$ is degree of node $i$\n",
    "\n",
    "2. **Centrality Measures**:\n",
    "- Degree centrality:\n",
    "$$\n",
    "C_D(i) = \\frac{k_i}{|V|-1}\n",
    "$$\n",
    "- Betweenness centrality:\n",
    "$$\n",
    "C_B(i) = \\sum_{s \\neq i \\neq t} \\frac{\\sigma_{st}(i)}{\\sigma_{st}}\n",
    "$$\n",
    "where $\\sigma_{st}$ is number of shortest paths between $s,t$, and $\\sigma_{st}(i)$ is number passing through $i$\n",
    "\n",
    "**Community Detection**\n",
    "\n",
    "Using Louvain method for modularity optimization:\n",
    "\n",
    "1. **Modularity Function**:\n",
    "$$\n",
    "Q = \\frac{1}{2m}\\sum_{ij} \\left[A_{ij} - \\frac{k_i k_j}{2m}\\right]\\delta(c_i,c_j)\n",
    "$$\n",
    "where:\n",
    "- $A_{ij}$ is adjacency matrix\n",
    "- $k_i, k_j$ are node degrees\n",
    "- $m$ is total edge weight\n",
    "- $c_i, c_j$ are community labels\n",
    "- $\\delta(c_i,c_j)$ is Kronecker delta\n",
    "\n",
    "2. **Optimization Process**:\n",
    "- Phase 1: Iteratively move nodes to maximize local modularity gain\n",
    "$$\n",
    "\\Delta Q = \\left[\\frac{\\Sigma_{in} + k_{i,in}}{2m} - \\left(\\frac{\\Sigma_{tot} + k_i}{2m}\\right)^2\\right] - \\left[\\frac{\\Sigma_{in}}{2m} - \\left(\\frac{\\Sigma_{tot}}{2m}\\right)^2 - \\left(\\frac{k_i}{2m}\\right)^2\\right]\n",
    "$$\n",
    "where:\n",
    "- $\\Sigma_{in}$ is sum of weights inside community\n",
    "- $\\Sigma_{tot}$ is sum of weights of links incident to nodes in community\n",
    "- $k_{i,in}$ is sum of weights of links from $i$ to nodes in community\n",
    "- $k_i$ is sum of weights of links incident to node $i$\n",
    "\n",
    "- Phase 2: Aggregate nodes in same community and repeat\n",
    "\n",
    "The algorithm iterates these phases until no improvement in modularity is possible.\n",
    "\n",
    "This mathematical framework:\n",
    "1. Creates meaningful patient similarities\n",
    "2. Builds network structure\n",
    "3. Identifies important patients\n",
    "4. Discovers natural patient groupings\n",
    "\n",
    "The parameters (threshold, k_nearest) control network sparsity and community granularity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Suggest five student projects for further exploration and improvements of this PSN approach using the present data defined by the pandas dataframe df._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are five suggested student projects for exploring and improving the PSN approach:\n",
    "\n",
    "1. **Feature Selection and Weighting Project**\n",
    "```python\n",
    "\"\"\"\n",
    "Project Goal: Optimize feature importance in PSN construction\n",
    "\n",
    "Key Steps:\n",
    "1. Feature importance analysis:\n",
    "   - Use random forests to rank features\n",
    "   - Calculate correlation matrices\n",
    "   - Implement PCA/factor analysis\n",
    "\n",
    "2. Weighted similarity measures:\n",
    "   - Modify distance calculation:\n",
    "   d_ij = sqrt(sum(w_k * (x_ik - x_jk)^2))\n",
    "   where w_k are feature weights\n",
    "\n",
    "3. Compare network structures:\n",
    "   - Original vs weighted features\n",
    "   - Different feature subsets\n",
    "   - Impact on community detection\n",
    "\n",
    "4. Validation metrics:\n",
    "   - Community stability\n",
    "   - Classification accuracy\n",
    "   - Clinical relevance of groupings\n",
    "\"\"\"\n",
    "\n",
    "def weighted_similarity_network(df, feature_weights):\n",
    "    # Implementation here\n",
    "    pass\n",
    "```\n",
    "\n",
    "2. **Dynamic PSN Analysis Project**\n",
    "```python\n",
    "\"\"\"\n",
    "Project Goal: Analyze network stability and robustness\n",
    "\n",
    "Key Steps:\n",
    "1. Bootstrap analysis:\n",
    "   - Random subsampling of patients\n",
    "   - Perturbation of feature values\n",
    "   - Multiple network constructions\n",
    "\n",
    "2. Stability metrics:\n",
    "   - Community persistence\n",
    "   - Node role consistency\n",
    "   - Edge reliability\n",
    "\n",
    "3. Parameter sensitivity:\n",
    "   - Impact of threshold values\n",
    "   - Effect of k_nearest changes\n",
    "   - Similarity measure variations\n",
    "\n",
    "4. Visualization:\n",
    "   - Confidence intervals for edges\n",
    "   - Node role probability\n",
    "   - Community overlap diagrams\n",
    "\"\"\"\n",
    "\n",
    "def network_stability_analysis(df, n_iterations=100):\n",
    "    # Implementation here\n",
    "    pass\n",
    "```\n",
    "\n",
    "3. **Clinical Subtype Discovery Project**\n",
    "```python\n",
    "\"\"\"\n",
    "Project Goal: Identify and validate clinical subtypes\n",
    "\n",
    "Key Steps:\n",
    "1. Enhanced community detection:\n",
    "   - Multiple algorithms comparison\n",
    "   - Hierarchical community structure\n",
    "   - Overlapping communities\n",
    "\n",
    "2. Clinical characterization:\n",
    "   - Detailed demographic analysis\n",
    "   - Symptom pattern analysis\n",
    "   - Outcome correlations\n",
    "\n",
    "3. Validation:\n",
    "   - External clinical criteria\n",
    "   - Literature comparison\n",
    "   - Expert clinician review\n",
    "\n",
    "4. Subtype prediction:\n",
    "   - New patient classification\n",
    "   - Confidence measures\n",
    "   - Clinical interpretation\n",
    "\"\"\"\n",
    "\n",
    "def clinical_subtype_analysis(df, communities):\n",
    "    # Implementation here\n",
    "    pass\n",
    "```\n",
    "\n",
    "4. **Multi-Layer PSN Project**\n",
    "```python\n",
    "\"\"\"\n",
    "Project Goal: Create and analyze multi-layer patient networks\n",
    "\n",
    "Key Steps:\n",
    "1. Layer definition:\n",
    "   - Morphometric features\n",
    "   - Cognitive measures\n",
    "   - Clinical symptoms\n",
    "   - Demographics\n",
    "\n",
    "2. Layer integration:\n",
    "   - Inter-layer edges\n",
    "   - Layer weights\n",
    "   - Aggregate measures\n",
    "\n",
    "3. Multi-layer analysis:\n",
    "   - Layer-specific communities\n",
    "   - Cross-layer patterns\n",
    "   - Node role across layers\n",
    "\n",
    "4. Clinical applications:\n",
    "   - Multi-aspect patient similarity\n",
    "   - Comprehensive subtyping\n",
    "   - Layer importance for outcomes\n",
    "\"\"\"\n",
    "\n",
    "def multilayer_network_analysis(df):\n",
    "    # Implementation here\n",
    "    pass\n",
    "```\n",
    "\n",
    "5. **Predictive PSN Project**\n",
    "```python\n",
    "\"\"\"\n",
    "Project Goal: Develop predictive models using PSN structure\n",
    "\n",
    "Key Steps:\n",
    "1. Network-based features:\n",
    "   - Node embeddings\n",
    "   - Community memberships\n",
    "   - Centrality measures\n",
    "   - Path-based features\n",
    "\n",
    "2. Prediction tasks:\n",
    "   - IBS severity\n",
    "   - Treatment response\n",
    "   - Symptom progression\n",
    "   - Comorbidity risk\n",
    "\n",
    "3. Model development:\n",
    "   - Graph neural networks\n",
    "   - Network-aware classifiers\n",
    "   - Ensemble methods\n",
    "   - Uncertainty estimation\n",
    "\n",
    "4. Clinical validation:\n",
    "   - Prediction accuracy\n",
    "   - Feature importance\n",
    "   - Clinical interpretability\n",
    "   - Practical utility\n",
    "\"\"\"\n",
    "\n",
    "def network_based_prediction(df, G, target_variable):\n",
    "    # Implementation here\n",
    "    pass\n",
    "```\n",
    "\n",
    "Each project:\n",
    "- Has clear objectives\n",
    "- Builds on existing code\n",
    "- Addresses important clinical questions\n",
    "- Combines theory and practice\n",
    "- Produces actionable results\n",
    "\n",
    "The projects can be scaled based on:\n",
    "- Student level (undergraduate/graduate)\n",
    "- Time available (semester/year)\n",
    "- Technical expertise\n",
    "- Clinical focus\n",
    "\n",
    "Required skills include:\n",
    "- Python programming\n",
    "- Network analysis\n",
    "- Statistical methods\n",
    "- Clinical knowledge\n",
    "- Data visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Rank each of the five projects according to diffuculty - conceptially and technically._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a ranking of the five projects by difficulty (1=least difficult, 5=most difficult):\n",
    "\n",
    "**Conceptual Difficulty Ranking:**\n",
    "\n",
    "1. **Feature Selection and Weighting Project**\n",
    "   - Familiar statistical concepts\n",
    "   - Clear objective (feature importance)\n",
    "   - Well-established methods\n",
    "   - Straightforward validation\n",
    "   - Direct interpretation\n",
    "\n",
    "2. **Dynamic PSN Analysis Project**\n",
    "   - Bootstrap concepts are intuitive\n",
    "   - Clear stability metrics\n",
    "   - Visual interpretation\n",
    "   - Standard statistical tests\n",
    "   - Familiar validation methods\n",
    "\n",
    "3. **Clinical Subtype Discovery Project**\n",
    "   - Requires clinical knowledge\n",
    "   - Community detection concepts\n",
    "   - Validation complexity\n",
    "   - Multiple algorithm comparison\n",
    "   - Clinical interpretation needed\n",
    "\n",
    "4. **Predictive PSN Project**\n",
    "   - Complex prediction tasks\n",
    "   - Model selection challenges\n",
    "   - Uncertainty quantification\n",
    "   - Clinical validation complexity\n",
    "   - Feature engineering concepts\n",
    "\n",
    "5. **Multi-Layer PSN Project**\n",
    "   - Abstract network concepts\n",
    "   - Layer integration complexity\n",
    "   - Multi-dimensional thinking\n",
    "   - Complex interpretation\n",
    "   - Advanced network theory\n",
    "\n",
    "**Technical Difficulty Ranking:**\n",
    "\n",
    "1. **Dynamic PSN Analysis Project**\n",
    "   - Standard Python libraries\n",
    "   - Basic network metrics\n",
    "   - Simple visualizations\n",
    "   - Straightforward iterations\n",
    "   - Basic statistical tests\n",
    "\n",
    "2. **Feature Selection and Weighting Project**\n",
    "   - Standard ML libraries\n",
    "   - Matrix operations\n",
    "   - Basic optimization\n",
    "   - Common visualization tools\n",
    "   - Standard validation methods\n",
    "\n",
    "3. **Clinical Subtype Discovery Project**\n",
    "   - Multiple algorithms\n",
    "   - Community detection implementation\n",
    "   - Statistical validation\n",
    "   - Advanced visualization\n",
    "   - Data integration\n",
    "\n",
    "4. **Multi-Layer PSN Project**\n",
    "   - Complex data structures\n",
    "   - Custom network metrics\n",
    "   - Layer integration code\n",
    "   - Advanced visualization\n",
    "   - Performance optimization\n",
    "\n",
    "5. **Predictive PSN Project**\n",
    "   - Graph neural networks\n",
    "   - Advanced ML frameworks\n",
    "   - Custom loss functions\n",
    "   - Complex model architectures\n",
    "   - Performance optimization\n",
    "\n",
    "**Combined Considerations:**\n",
    "1. **Time Requirements:**\n",
    "   - Feature Selection: 1-2 months\n",
    "   - Dynamic Analysis: 2-3 months\n",
    "   - Clinical Subtypes: 3-4 months\n",
    "   - Multi-Layer PSN: 4-6 months\n",
    "   - Predictive PSN: 4-6 months\n",
    "\n",
    "2. **Required Background:**\n",
    "   - Feature Selection: Basic statistics, ML\n",
    "   - Dynamic Analysis: Basic networks, statistics\n",
    "   - Clinical Subtypes: Networks, clinical knowledge\n",
    "   - Multi-Layer PSN: Advanced networks, math\n",
    "   - Predictive PSN: Deep learning, advanced ML\n",
    "\n",
    "3. **Resource Needs:**\n",
    "   - Feature Selection: Standard computing\n",
    "   - Dynamic Analysis: Standard computing\n",
    "   - Clinical Subtypes: Standard computing, clinical data\n",
    "   - Multi-Layer PSN: High memory, possibly GPU\n",
    "   - Predictive PSN: GPU computing, large datasets\n",
    "\n",
    "4. **Recommended For:**\n",
    "   - Feature Selection: Advanced undergrad\n",
    "   - Dynamic Analysis: Advanced undergrad\n",
    "   - Clinical Subtypes: Early graduate\n",
    "   - Multi-Layer PSN: Advanced graduate\n",
    "   - Predictive PSN: Advanced graduate\n",
    "\n",
    "This ranking helps in:\n",
    "- Project assignment based on student level\n",
    "- Resource allocation\n",
    "- Timeline planning\n",
    "- Support requirements\n",
    "- Expected outcomes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_It seems that all the suggested projects are too difficult for the medical students in the course or too time consuming. Most students have little experience in Python programming or in mathematics, and they only have 2 weeks to complete the project, working in groups of 4 students. Could you suggest five simpler projects related to our PSN methodology being suitable regarding modest technical skills and short time frame?_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Here are five simpler projects suitable for medical students with basic programming skills and a 2-week timeframe:\n",
    "\n",
    "1. **Clinical Interpretation of Communities Project**\n",
    "````python\n",
    "\"\"\"\n",
    "Project Goal: Analyze and interpret patient communities from clinical perspective\n",
    "\n",
    "Key Steps:\n",
    "1. For each community:\n",
    "   - Summarize basic demographics (age, gender)\n",
    "   - Compare IBS severity scores\n",
    "   - Analyze education levels\n",
    "   - Count IBS vs HC ratios\n",
    "\n",
    "2. Create simple visualizations:\n",
    "   - Bar charts of key variables\n",
    "   - Box plots comparing groups\n",
    "   - Pie charts of distributions\n",
    "\n",
    "3. Clinical interpretation:\n",
    "   - Describe patient profiles in each community\n",
    "   - Identify distinguishing characteristics\n",
    "   - Suggest clinical relevance\n",
    "\n",
    "Time: 2 weeks\n",
    "Group size: 4 students\n",
    "Skills needed: Basic Python (pandas, matplotlib)\n",
    "\"\"\"\n",
    "\n",
    "# Example starter code\n",
    "def analyze_community_characteristics(df, communities):\n",
    "    for i, community in enumerate(communities):\n",
    "        community_df = df.iloc[list(community)]\n",
    "        \n",
    "        print(f\"\\nCommunity {i+1} Analysis:\")\n",
    "        print(\"------------------------\")\n",
    "        print(f\"Size: {len(community)} patients\")\n",
    "        print(f\"IBS patients: {sum(community_df['Group']=='IBS')}\")\n",
    "        print(f\"HC patients: {sum(community_df['Group']=='HC')}\")\n",
    "        print(f\"Average age: {community_df['Age'].mean():.1f}\")\n",
    "        # Add more analysis as needed\n",
    "````\n",
    "\n",
    "2. **Network Visualization Enhancement Project**\n",
    "````python\n",
    "\"\"\"\n",
    "Project Goal: Create clinically meaningful visualizations of the PSN\n",
    "\n",
    "Key Steps:\n",
    "1. Color coding by different variables:\n",
    "   - IBS severity\n",
    "   - Age groups\n",
    "   - Gender\n",
    "   - Education levels\n",
    "\n",
    "2. Node labeling improvements:\n",
    "   - Add clinical annotations\n",
    "   - Highlight key patients\n",
    "   - Mark community boundaries\n",
    "\n",
    "3. Create interactive plots:\n",
    "   - Hover information\n",
    "   - Zoom capabilities\n",
    "   - Filtering options\n",
    "\n",
    "Time: 2 weeks\n",
    "Group size: 4 students\n",
    "Skills needed: Basic Python (networkx, plotly)\n",
    "\"\"\"\n",
    "\n",
    "# Example starter code\n",
    "def visualize_network_by_variable(G, df, color_variable):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    # Add visualization code\n",
    "    # Color nodes by selected variable\n",
    "    # Add labels and legend\n",
    "````\n",
    "\n",
    "3. **Patient Similarity Analysis Project**\n",
    "````python\n",
    "\"\"\"\n",
    "Project Goal: Investigate what makes patients similar or different\n",
    "\n",
    "Key Steps:\n",
    "1. Select patient pairs:\n",
    "   - Most similar pairs\n",
    "   - Most different pairs\n",
    "   - Same/different groups\n",
    "\n",
    "2. Compare characteristics:\n",
    "   - Create comparison tables\n",
    "   - Calculate key differences\n",
    "   - Visualize patterns\n",
    "\n",
    "3. Clinical relevance:\n",
    "   - Identify key distinguishing features\n",
    "   - Suggest clinical implications\n",
    "   - Document findings\n",
    "\n",
    "Time: 2 weeks\n",
    "Group size: 4 students\n",
    "Skills needed: Basic Python (pandas, basic statistics)\n",
    "\"\"\"\n",
    "\n",
    "# Example starter code\n",
    "def compare_patient_pairs(df, similarity_matrix):\n",
    "    # Find most/least similar pairs\n",
    "    # Create comparison tables\n",
    "    # Generate simple visualizations\n",
    "````\n",
    "\n",
    "4. **Demographic Impact Project**\n",
    "````python\n",
    "\"\"\"\n",
    "Project Goal: Study how demographics affect network structure\n",
    "\n",
    "Key Steps:\n",
    "1. Basic analysis:\n",
    "   - Age distribution in communities\n",
    "   - Gender patterns\n",
    "   - Education level effects\n",
    "\n",
    "2. Simple visualizations:\n",
    "   - Age histograms by community\n",
    "   - Gender proportion charts\n",
    "   - Education level comparisons\n",
    "\n",
    "3. Write clinical summary:\n",
    "   - Key demographic patterns\n",
    "   - Community characteristics\n",
    "   - Clinical implications\n",
    "\n",
    "Time: 2 weeks\n",
    "Group size: 4 students\n",
    "Skills needed: Basic Python (pandas, basic plotting)\n",
    "\"\"\"\n",
    "\n",
    "# Example starter code\n",
    "def analyze_demographics(df, communities):\n",
    "    # Create demographic summaries\n",
    "    # Generate simple plots\n",
    "    # Calculate basic statistics\n",
    "````\n",
    "\n",
    "5. **IBS Severity Pattern Project**\n",
    "````python\n",
    "\"\"\"\n",
    "Project Goal: Analyze IBS severity patterns in the network\n",
    "\n",
    "Key Steps:\n",
    "1. Basic analysis:\n",
    "   - IBS_SSS distribution\n",
    "   - Severity by community\n",
    "   - Relationship with other variables\n",
    "\n",
    "2. Simple visualizations:\n",
    "   - Severity histograms\n",
    "   - Box plots by community\n",
    "   - Basic correlation plots\n",
    "\n",
    "3. Clinical interpretation:\n",
    "   - Severity patterns\n",
    "   - Community characteristics\n",
    "   - Treatment implications\n",
    "\n",
    "Time: 2 weeks\n",
    "Group size: 4 students\n",
    "Skills needed: Basic Python (pandas, basic statistics)\n",
    "\"\"\"\n",
    "\n",
    "# Example starter code\n",
    "def analyze_ibs_patterns(df, communities):\n",
    "    # Analyze IBS severity scores\n",
    "    # Create basic visualizations\n",
    "    # Generate summary statistics\n",
    "````\n",
    "\n",
    "**Key Features of These Projects:**\n",
    "1. **Simpler Technical Requirements:**\n",
    "   - Basic Python usage\n",
    "   - Pre-written code templates\n",
    "   - Focus on analysis over programming\n",
    "\n",
    "2. **Clinical Focus:**\n",
    "   - Emphasis on medical interpretation\n",
    "   - Relevant to medical training\n",
    "   - Clear clinical applications\n",
    "\n",
    "3. **Manageable Scope:**\n",
    "   - Clear, limited objectives\n",
    "   - 2-week timeframe\n",
    "   - Group work friendly\n",
    "\n",
    "4. **Learning Value:**\n",
    "   - Practical data analysis\n",
    "   - Clinical interpretation\n",
    "   - Basic visualization skills\n",
    "\n",
    "5. **Support Needs:**\n",
    "   - Template code provided\n",
    "   - Clear step-by-step tasks\n",
    "   - Focused objectives\n",
    "\n",
    "These projects allow students to:\n",
    "- Focus on clinical interpretation\n",
    "- Work with familiar medical concepts\n",
    "- Use basic technical skills\n",
    "- Produce meaningful results\n",
    "- Complete work in available time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Please suggest a complete solution to project 5 (IBS Severity Pattern Project), followed by a report (Introduction, Methods, Results, Discussing and the code in an appendix). The report should be written in Markdown with an option to write in Latex (help students with this part)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll help break this down into parts. First, let's create the complete solution for Project 5, then the report in both Markdown and LaTeX formats.\n",
    "\n",
    "First, here's the complete code solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mTypeError: type 'List' is not subscriptable. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "def analyze_ibs_patterns(df, communities):\n",
    "    \"\"\"\n",
    "    Analyze IBS severity patterns in patient similarity network\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        Patient data including IBS_SSS scores\n",
    "    communities : list\n",
    "        List of sets containing node indices for each community\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Analysis results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Create results directory if it doesn't exist\n",
    "    !mkdir -p results\n",
    "\n",
    "    # 1. Overall IBS Severity Analysis\n",
    "    ibs_patients = df[df['Group'] == 'IBS']\n",
    "    results['overall_stats'] = {\n",
    "        'mean': ibs_patients['IBS_SSS'].mean(),\n",
    "        'std': ibs_patients['IBS_SSS'].std(),\n",
    "        'median': ibs_patients['IBS_SSS'].median(),\n",
    "        'q1': ibs_patients['IBS_SSS'].quantile(0.25),\n",
    "        'q3': ibs_patients['IBS_SSS'].quantile(0.75)\n",
    "    }\n",
    "    \n",
    "    # 2. Community-based Analysis\n",
    "    community_stats = []\n",
    "    for i, community in enumerate(communities):\n",
    "        community_df = df.iloc[list(community)]\n",
    "        ibs_in_community = community_df[community_df['Group'] == 'IBS']\n",
    "        \n",
    "        if len(ibs_in_community) > 0:\n",
    "            stats = {\n",
    "                'community': i,\n",
    "                'n_ibs': len(ibs_in_community),\n",
    "                'mean_severity': ibs_in_community['IBS_SSS'].mean(),\n",
    "                'std_severity': ibs_in_community['IBS_SSS'].std(),\n",
    "                'median_severity': ibs_in_community['IBS_SSS'].median()\n",
    "            }\n",
    "            community_stats.append(stats)\n",
    "    \n",
    "    results['community_stats'] = pd.DataFrame(community_stats)\n",
    "    \n",
    "    # 3. Correlation Analysis\n",
    "    correlation_vars = ['Age', 'Education', 'IBS_SSS']\n",
    "    correlations = ibs_patients[correlation_vars].corr()\n",
    "    results['correlations'] = correlations\n",
    "    \n",
    "    # 4. Visualizations\n",
    "    # 4.1 Overall Distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data=ibs_patients, x='IBS_SSS', bins=20)\n",
    "    plt.title('Distribution of IBS Severity Scores')\n",
    "    plt.xlabel('IBS Severity Score')\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig('./results/ibs_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4.2 Severity by Community\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    community_data = []\n",
    "    community_labels = []\n",
    "    for i, community in enumerate(communities):\n",
    "        community_df = df.iloc[list(community)]\n",
    "        ibs_data = community_df[community_df['Group'] == 'IBS']['IBS_SSS']\n",
    "        if len(ibs_data) > 0:\n",
    "            community_data.append(ibs_data)\n",
    "            community_labels.append(f'Community {i}')\n",
    "    \n",
    "    plt.boxplot(community_data, labels=community_labels)\n",
    "    plt.title('IBS Severity Scores by Community')\n",
    "    plt.ylabel('IBS Severity Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.savefig('./results/severity_by_community.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4.3 Correlation Heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(correlations, annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.savefig('./results/correlation_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def generate_severity_report(results):\n",
    "    \"\"\"Generate summary statistics and key findings\"\"\"\n",
    "    report = {\n",
    "        'overall_summary': {\n",
    "            'mean_severity': results['overall_stats']['mean'],\n",
    "            'severity_range': (\n",
    "                results['overall_stats']['q1'],\n",
    "                results['overall_stats']['q3']\n",
    "            )\n",
    "        },\n",
    "        'community_findings': {\n",
    "            'n_communities': len(results['community_stats']),\n",
    "            'max_severity_community': results['community_stats']['mean_severity'].idxmax(),\n",
    "            'min_severity_community': results['community_stats']['mean_severity'].idxmin()\n",
    "        },\n",
    "        'correlations': {\n",
    "            'age_correlation': results['correlations'].loc['Age', 'IBS_SSS'],\n",
    "            'education_correlation': results['correlations'].loc['Education', 'IBS_SSS']\n",
    "        }\n",
    "    }\n",
    "    return report\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data and run analysis\n",
    "    # df = pd.read_csv('your_data.csv')\n",
    "    # communities = your_community_detection_results\n",
    "    # results = analyze_ibs_patterns(df, communities)\n",
    "    # report = generate_severity_report(results)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Please complete the code in __main__ such that it can be excuted and produce sensible results._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mTypeError: type 'List' is not subscriptable. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 1. Load data\n",
    "    df = pd.read_csv('https://raw.githubusercontent.com/arvidl/ibs-brain/main/data/demographics_fs7_rbans_IBS_SSS_imputed_78x48.csv')\n",
    "    \n",
    "    # 2. Create similarity network (simplified version)\n",
    "    def create_simple_network(df, threshold=0.3, k_nearest=8):\n",
    "        # Prepare data for network creation\n",
    "        exclude_cols = ['Subject', 'Group', 'IBS_SSS', 'Age', 'Gender', 'Education']\n",
    "        feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "        \n",
    "        # Scale the data\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        scaled_data = StandardScaler().fit_transform(df[feature_cols])\n",
    "        \n",
    "        # Calculate similarities\n",
    "        from scipy.spatial.distance import pdist, squareform\n",
    "        distances = squareform(pdist(scaled_data, 'euclidean'))\n",
    "        similarities = np.exp(-distances ** 2 / (2 * np.mean(distances) ** 2))\n",
    "        \n",
    "        # Create network\n",
    "        import networkx as nx\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Add nodes\n",
    "        for i in range(len(df)):\n",
    "            G.add_node(i)\n",
    "        \n",
    "        # Add edges based on similarity threshold and k-nearest neighbors\n",
    "        for i in range(len(df)):\n",
    "            # Get k nearest neighbors\n",
    "            neighbors = np.argsort(similarities[i])[-k_nearest-1:-1]\n",
    "            for j in neighbors:\n",
    "                if similarities[i,j] > threshold:\n",
    "                    G.add_edge(i, j, weight=similarities[i,j])\n",
    "        \n",
    "        return G, similarities\n",
    "    \n",
    "    # 3. Create network and detect communities\n",
    "    G, similarities = create_simple_network(df)\n",
    "    \n",
    "    # Detect communities using Louvain method\n",
    "    import community.community_louvain as community_louvain\n",
    "    communities_dict = community_louvain.best_partition(G)\n",
    "    \n",
    "    # Convert communities dict to list of sets\n",
    "    max_comm = max(communities_dict.values())\n",
    "    communities = [set() for _ in range(max_comm + 1)]\n",
    "    for node, comm in communities_dict.items():\n",
    "        communities[comm].add(node)\n",
    "    \n",
    "    # 4. Run analysis\n",
    "    results = analyze_ibs_patterns(df, communities)\n",
    "    report = generate_severity_report(results)\n",
    "    \n",
    "    # 5. Print key findings\n",
    "    print(\"\\nKey Findings:\")\n",
    "    print(\"============\")\n",
    "    print(f\"Number of communities: {report['community_findings']['n_communities']}\")\n",
    "    print(\"\\nOverall IBS Severity:\")\n",
    "    print(f\"Mean: {report['overall_summary']['mean_severity']:.1f}\")\n",
    "    print(f\"IQR: {report['overall_summary']['severity_range'][0]:.1f} - \"\n",
    "          f\"{report['overall_summary']['severity_range'][1]:.1f}\")\n",
    "    \n",
    "    print(\"\\nCommunity Analysis:\")\n",
    "    print(\"Most severe community:\", report['community_findings']['max_severity_community'])\n",
    "    print(\"Least severe community:\", report['community_findings']['min_severity_community'])\n",
    "    \n",
    "    print(\"\\nCorrelations with IBS_SSS:\")\n",
    "    print(f\"Age: r = {report['correlations']['age_correlation']:.3f}\")\n",
    "    print(f\"Education: r = {report['correlations']['education_correlation']:.3f}\")\n",
    "    \n",
    "    # 6. Display community statistics\n",
    "    print(\"\\nDetailed Community Statistics:\")\n",
    "    print(results['community_stats'].to_string(index=False))\n",
    "    \n",
    "    # 7. Show that figures were created\n",
    "    print(\"\\nGenerated Figures:\")\n",
    "    print(\"1. ./results/ibs_distribution.png - Overall severity distribution\")\n",
    "    print(\"2. ./results/severity_by_community.png - Severity by community\")\n",
    "    print(\"3. ./results/correlation_matrix.png - Correlation heatmap\")\n",
    "\n",
    "# The script will now run when executed and produce:\n",
    "# - Printed statistics and findings\n",
    "# - Three visualization files\n",
    "# - Complete analysis of IBS severity patterns in the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to display the three figures in a Markdown cell:\n",
    "\n",
    "# Analysis Results\n",
    "\n",
    "## 1. Overall IBS Severity Distribution\n",
    "![IBS Severity Distribution](./results/ibs_distribution.png)\n",
    "*Figure 1: Distribution of IBS Severity Scores across all patients. The x-axis shows the IBS_SSS score ranges, and the y-axis shows the frequency of patients in each range.*\n",
    "\n",
    "## 2. Severity by Community\n",
    "![Severity by Community](./results/severity_by_community.png)\n",
    "*Figure 2: Box plots showing the distribution of IBS Severity Scores within each detected community. The boxes show the quartiles, whiskers show the range, and points indicate potential outliers.*\n",
    "\n",
    "## 3. Correlation Matrix\n",
    "![Correlation Matrix](./results/correlation_matrix.png)\n",
    "*Figure 3: Heatmap showing correlations between IBS Severity Score (IBS_SSS), Age, and Education level. Red indicates positive correlations, blue indicates negative correlations, and the intensity represents the strength of the correlation.*\n",
    "\n",
    "**Key Observations:**\n",
    "1. From Figure 1: [Add interpretation of the severity distribution]\n",
    "2. From Figure 2: [Add interpretation of community patterns]\n",
    "3. From Figure 3: [Add interpretation of correlations]\n",
    "\n",
    "Note: The actual images will appear when the code is run and the figures are generated in the specified directory. Make sure the `./results/` directory exists before running the analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Please fill in given the figures are generated in the specified directory._\n",
    "\n",
    "**NOTE: cursor with claude-3.5-sonnet can make intepretations of the figures!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the key observations from the three figures:\n",
    "\n",
    "**Key Observations:**\n",
    "\n",
    "1. **From Figure 1 (IBS Severity Distribution):**\n",
    "   - The distribution shows a primary peak around 250-275 on the IBS Severity Score scale\n",
    "   - There appears to be a bimodal distribution with a smaller secondary peak around 400\n",
    "   - The distribution is slightly right-skewed\n",
    "   - Most patients fall in the moderate severity range (200-300)\n",
    "   - There are relatively few patients with very low (<150) or very high (>400) severity scores\n",
    "\n",
    "2. **From Figure 2 (Severity by Community):**\n",
    "   - Four distinct communities were identified\n",
    "   - Community 2 shows the highest median severity and largest variability\n",
    "   - Community 4 has the lowest median severity and most compact distribution\n",
    "   - Communities 1 and 3 show intermediate severity levels\n",
    "   - Several outliers are present, particularly in Communities 1 and 2 (dots above boxes)\n",
    "   - The interquartile ranges (box sizes) vary considerably between communities, suggesting different levels of homogeneity\n",
    "\n",
    "3. **From Figure 3 (Correlation Matrix):**\n",
    "   - There is a moderate negative correlation (-0.18) between Age and IBS_SSS, suggesting slightly lower severity in older patients\n",
    "   - Education shows very weak correlations with both IBS_SSS (-0.02) and Age (-0.0043), indicating minimal relationship\n",
    "   - The correlations are generally weak, suggesting that demographic factors alone don't strongly predict IBS severity\n",
    "   - The strongest relationship observed is the negative Age-IBS_SSS correlation, though still relatively weak\n",
    "\n",
    "These patterns suggest that while community structure captures some meaningful groupings in IBS severity, demographic factors have limited direct correlation with symptom severity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I'll provide the report template in both Markdown and LaTeX formats in my next response, as it's quite extensive.\n",
    "\n",
    "Would you like me to proceed with the report templates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Yes, proceed with the report templates._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the report templates in both Markdown and LaTeX formats:\n",
    "\n",
    "**Markdown Version:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBS Severity Patterns in Patient Similarity Networks: A Community-Based Analysis\n",
    "\n",
    "## Abstract\n",
    "This study investigates the distribution and patterns of IBS severity scores within patient communities identified through network analysis. Using a dataset of {N} patients, we analyzed how IBS severity relates to community structure and demographic factors.\n",
    "\n",
    "## Introduction\n",
    "Irritable Bowel Syndrome (IBS) presents with varying degrees of severity across patient populations. Understanding these patterns within naturally occurring patient communities could provide insights into disease subtypes and patient stratification. This study leverages Patient Similarity Networks (PSN) to explore IBS severity patterns.\n",
    "\n",
    "### Research Questions\n",
    "1. How does IBS severity distribute across different patient communities?\n",
    "2. What relationships exist between severity scores and demographic factors?\n",
    "3. Can community structure inform our understanding of IBS severity patterns?\n",
    "\n",
    "## Methods\n",
    "### Data\n",
    "- Source: {dataset_source}\n",
    "- Sample size: {N} patients ({n_ibs} IBS, {n_hc} HC)\n",
    "- Variables analyzed:\n",
    "  - IBS Severity Score (IBS_SSS)\n",
    "  - Demographic factors (Age, Gender, Education)\n",
    "  - Community assignments from PSN analysis\n",
    "\n",
    "### Analysis Approach\n",
    "1. **Community-based Analysis**\n",
    "   - Calculated summary statistics for each community\n",
    "   - Compared severity distributions between communities\n",
    "   - Analyzed demographic composition\n",
    "\n",
    "2. **Statistical Methods**\n",
    "   - Descriptive statistics\n",
    "   - Correlation analysis\n",
    "   - Between-community comparisons\n",
    "\n",
    "3. **Visualization**\n",
    "   - Distribution plots\n",
    "   - Box plots by community\n",
    "   - Correlation heatmaps\n",
    "\n",
    "## Results\n",
    "### Overall Severity Distribution\n",
    "- Mean IBS_SSS: {mean_severity:.1f} \u00b1 {std_severity:.1f}\n",
    "- Median [IQR]: {median_severity:.1f} [{q1:.1f}-{q3:.1f}]\n",
    "- Range: {min_severity:.1f}-{max_severity:.1f}\n",
    "\n",
    "### Community Analysis\n",
    "- Number of communities: {n_communities}\n",
    "- Community size range: {min_size}-{max_size} patients\n",
    "- Severity variations between communities:\n",
    "  - Highest severity: Community {max_sev_comm} ({max_sev:.1f})\n",
    "  - Lowest severity: Community {min_sev_comm} ({min_sev:.1f})\n",
    "\n",
    "### Correlations\n",
    "- Age correlation: r = {age_corr:.2f}, p = {age_p:.3f}\n",
    "- Education correlation: r = {edu_corr:.2f}, p = {edu_p:.3f}\n",
    "\n",
    "## Discussion\n",
    "### Key Findings\n",
    "1. [Main finding about severity distribution]\n",
    "2. [Main finding about community patterns]\n",
    "3. [Main finding about correlations]\n",
    "\n",
    "### Clinical Implications\n",
    "- [Implication 1]\n",
    "- [Implication 2]\n",
    "- [Implication 3]\n",
    "\n",
    "### Limitations\n",
    "- [Limitation 1]\n",
    "- [Limitation 2]\n",
    "- [Limitation 3]\n",
    "\n",
    "### Future Directions\n",
    "- [Suggestion 1]\n",
    "- [Suggestion 2]\n",
    "- [Suggestion 3]\n",
    "\n",
    "## Conclusion\n",
    "[Brief summary of main findings and implications]\n",
    "\n",
    "## References\n",
    "1. [Reference 1]\n",
    "2. [Reference 2]\n",
    "3. [Reference 3]\n",
    "\n",
    "## Appendix: Code\n",
    "```python\n",
    "[Insert complete code here]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LaTeX Version:**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\documentclass[12pt]{article}\n",
    "\n",
    "\\usepackage{amsmath}\n",
    "\\usepackage{graphicx}\n",
    "\\usepackage{booktabs}\n",
    "\\usepackage{float}\n",
    "\\usepackage{hyperref}\n",
    "\n",
    "\\title{IBS Severity Patterns in Patient Similarity Networks: \\\\ A Community-Based Analysis}\n",
    "\\author{[Author Names]}\n",
    "\\date{\\today}\n",
    "\n",
    "\\begin{document}\n",
    "\\maketitle\n",
    "\n",
    "\\begin{abstract}\n",
    "This study investigates the distribution and patterns of IBS severity scores within patient communities identified through network analysis. Using a dataset of $N$ patients, we analyzed how IBS severity relates to community structure and demographic factors.\n",
    "\\end{abstract}\n",
    "\n",
    "\\section{Introduction}\n",
    "Irritable Bowel Syndrome (IBS) presents with varying degrees of severity across patient populations. Understanding these patterns within naturally occurring patient communities could provide insights into disease subtypes and patient stratification.\n",
    "\n",
    "\\subsection{Research Questions}\n",
    "\\begin{enumerate}\n",
    "    \\item How does IBS severity distribute across different patient communities?\n",
    "    \\item What relationships exist between severity scores and demographic factors?\n",
    "    \\item Can community structure inform our understanding of IBS severity patterns?\n",
    "\\end{enumerate}\n",
    "\n",
    "\\section{Methods}\n",
    "\\subsection{Data}\n",
    "\\begin{itemize}\n",
    "    \\item Source: [dataset source]\n",
    "    \\item Sample size: $N$ patients ($n_{IBS}$ IBS, $n_{HC}$ HC)\n",
    "    \\item Variables analyzed:\n",
    "    \\begin{itemize}\n",
    "        \\item IBS Severity Score (IBS\\_SSS)\n",
    "        \\item Demographic factors (Age, Gender, Education)\n",
    "        \\item Community assignments from PSN analysis\n",
    "    \\end{itemize}\n",
    "\\end{itemize}\n",
    "\n",
    "[Continue with remaining sections...]\n",
    "\n",
    "\\section{Results}\n",
    "\\subsection{Overall Severity Distribution}\n",
    "\\begin{equation}\n",
    "\\text{Mean IBS\\_SSS} = \\mu \\pm \\sigma\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{table}[H]\n",
    "\\centering\n",
    "\\caption{Community Statistics}\n",
    "\\begin{tabular}{lrrr}\n",
    "\\toprule\n",
    "Community & Size & Mean Severity & SD \\\\\n",
    "\\midrule\n",
    "1 & x & y & z \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\n",
    "\\section{Discussion}\n",
    "[Discussion content...]\n",
    "\n",
    "\\appendix\n",
    "\\section{Code Implementation}\n",
    "\\begin{verbatim}\n",
    "[Insert complete code here]\n",
    "\\end{verbatim}\n",
    "\n",
    "\\end{document}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes for Students:**\n",
    "- Fill in the placeholders (in {}) with actual values\n",
    "- Add relevant figures from the analysis\n",
    "- Customize sections based on your specific findings\n",
    "\n",
    "**For LaTeX:**\n",
    "- Install a LaTeX distribution (e.g., TeX Live, MiKTeX)\n",
    "- Use an editor like Overleaf or TeXmaker\n",
    "Compile with pdfLaTeX\n",
    "Would you like me to provide more specific guidance on any section?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Describe the community detection using Louvain method - it's rationale, it's principles, it's mathematics_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a detailed explanation of the Louvain method for community detection:\n",
    "\n",
    "**1. Rationale**\n",
    "The Louvain method aims to find communities by optimizing modularity, which measures the density of links inside communities compared to links between communities. It's popular because it:\n",
    "- Is computationally efficient\n",
    "- Works well on large networks\n",
    "- Produces hierarchical communities\n",
    "- Automatically determines number of communities\n",
    "\n",
    "**2. Mathematical Framework**\n",
    "\n",
    "**Modularity (Q)**\n",
    "$$\n",
    "Q = \\frac{1}{2m} \\sum_{ij} \\left[A_{ij} - \\frac{k_i k_j}{2m}\\right] \\delta(c_i,c_j)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $A_{ij}$ is the edge weight between nodes i and j\n",
    "- $k_i = \\sum_j A_{ij}$ is the sum of weights of edges attached to node i\n",
    "- $m = \\frac{1}{2}\\sum_{ij} A_{ij}$ is the sum of all edge weights\n",
    "- $c_i$ is the community of node i\n",
    "- $\\delta(c_i,c_j)$ is 1 if $c_i = c_j$, and 0 otherwise\n",
    "\n",
    "**3. Algorithm Steps**\n",
    "\n",
    "**Phase 1: Modularity Optimization**\n",
    "```python\n",
    "# Pseudocode for Phase 1\n",
    "for each node i:\n",
    "    for each neighbor j of i:\n",
    "        # Calculate modularity gain of moving i to j's community\n",
    "        \u0394Q = gain_from_move(i, current_community, new_community)\n",
    "        if \u0394Q > 0:\n",
    "            move_node(i, new_community)\n",
    "```\n",
    "\n",
    "The modularity gain is calculated as:\n",
    "$$\n",
    "\\Delta Q = \\left[\\frac{\\Sigma_{in} + k_{i,in}}{2m} - \\left(\\frac{\\Sigma_{tot} + k_i}{2m}\\right)^2\\right] - \\left[\\frac{\\Sigma_{in}}{2m} - \\left(\\frac{\\Sigma_{tot}}{2m}\\right)^2 - \\left(\\frac{k_i}{2m}\\right)^2\\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\Sigma_{in}$ = sum of weights inside community\n",
    "- $\\Sigma_{tot}$ = sum of weights of links incident to nodes in community\n",
    "- $k_{i,in}$ = sum of weights of links from i to nodes in community\n",
    "- $k_i$ = sum of weights of links incident to node i\n",
    "\n",
    "**Phase 2: Network Aggregation**\n",
    "```python\n",
    "# Pseudocode for Phase 2\n",
    "G_new = create_empty_graph()\n",
    "for each community c in current_partition:\n",
    "    # Create super-node representing community\n",
    "    add_node(G_new, c)\n",
    "    # Add weighted edges between super-nodes\n",
    "    for neighboring_community in get_neighbors(c):\n",
    "        weight = sum_of_edges(c, neighboring_community)\n",
    "        add_edge(G_new, c, neighboring_community, weight)\n",
    "```\n",
    "\n",
    "**4. Implementation in Our Context**\n",
    "`````python\n",
    "# Using python-louvain package\n",
    "import community.community_louvain as community_louvain\n",
    "\n",
    "def detect_communities(G):\n",
    "    # Detect communities\n",
    "    communities = community_louvain.best_partition(G)\n",
    "    \n",
    "    # Convert to list of sets format\n",
    "    max_comm = max(communities.values())\n",
    "    community_sets = [set() for _ in range(max_comm + 1)]\n",
    "    for node, comm in communities.items():\n",
    "        community_sets[comm].add(node)\n",
    "    \n",
    "    return community_sets, communities\n",
    "`````\n",
    "\n",
    "**5. Key Properties**\n",
    "\n",
    "1. **Resolution**\n",
    "- Detects communities at different scales\n",
    "- Natural hierarchy emerges\n",
    "$$\n",
    "\\text{Resolution} \\approx \\sqrt{\\frac{\\text{total\\_edge\\_weight}}{\\text{number\\_of\\_nodes}}}\n",
    "$$\n",
    "\n",
    "2. **Complexity**\n",
    "- Time complexity: $O(n \\log n)$\n",
    "- Space complexity: $O(n)$\n",
    "- Scales well to large networks\n",
    "\n",
    "3. **Quality Measures**\n",
    "$$\n",
    "\\text{Modularity Score} = Q \\in [-1, 1]\n",
    "\\text{where higher values indicate better partitioning}\n",
    "$$\n",
    "\n",
    "**6. Advantages for PSN Analysis**\n",
    "\n",
    "1. **Clinical Relevance**\n",
    "- Natural grouping of similar patients\n",
    "- Hierarchical structure matches clinical categorization\n",
    "- Robust to noise in medical data\n",
    "\n",
    "2. **Practical Benefits**\n",
    "- Fast computation\n",
    "- No need to specify number of communities\n",
    "- Interpretable results\n",
    "\n",
    "3. **Limitations**\n",
    "- Resolution limit\n",
    "- Degeneracy of solutions\n",
    "- Stochastic nature\n",
    "\n",
    "This method is particularly suitable for PSNs because:\n",
    "- It handles weighted networks naturally\n",
    "- It's computationally efficient for medical datasets\n",
    "- It produces interpretable patient communities\n",
    "- It allows for hierarchical patient stratification\n",
    "\n",
    "The communities detected can inform:\n",
    "- Patient subtyping\n",
    "- Treatment strategies\n",
    "- Clinical decision making\n",
    "- Research cohort selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Any final thoughts on the proposed student projects?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some final thoughts and recommendations for the student projects:\n",
    "\n",
    "**1. Project Selection and Planning**\n",
    "- Start with simpler projects (like IBS Severity Pattern Analysis)\n",
    "- Form balanced teams (mix clinical knowledge and technical skills)\n",
    "- Set realistic milestones for 2-week timeline\n",
    "- Focus on clinical interpretation over technical complexity\n",
    "\n",
    "**2. Practical Tips for Success**\n",
    "\n",
    "```python\n",
    "# Project Timeline Example\n",
    "Week 1:\n",
    "- Days 1-2: Data understanding and initial analysis\n",
    "- Days 3-4: Basic visualizations and statistics\n",
    "- Day 5: Team review and adjustments\n",
    "\n",
    "Week 2:\n",
    "- Days 1-2: Complete analysis and visualizations\n",
    "- Days 3-4: Report writing and interpretation\n",
    "- Day 5: Final presentation preparation\n",
    "```\n",
    "\n",
    "**3. Key Focus Areas**\n",
    "- Clinical relevance over mathematical complexity\n",
    "- Clear visualizations over complex algorithms\n",
    "- Team collaboration over individual coding\n",
    "- Quality interpretation over quantity of analysis\n",
    "\n",
    "**4. Common Pitfalls to Avoid**\n",
    "```python\n",
    "# DON'T:\n",
    "- Spend too much time on complex coding\n",
    "- Try to implement advanced algorithms\n",
    "- Get lost in technical details\n",
    "\n",
    "# DO:\n",
    "- Use provided code templates\n",
    "- Focus on clinical interpretation\n",
    "- Leverage team members' strengths\n",
    "- Ask for help early when needed\n",
    "```\n",
    "\n",
    "**5. Recommended Project Structure**\n",
    "```\n",
    "project/\n",
    "\u2502\n",
    "\u251c\u2500\u2500 data/                 # Data files\n",
    "\u251c\u2500\u2500 analysis/            # Analysis scripts\n",
    "\u251c\u2500\u2500 figures/             # Generated plots\n",
    "\u251c\u2500\u2500 report/              # Final report\n",
    "\u2514\u2500\u2500 presentation/        # Presentation slides\n",
    "```\n",
    "\n",
    "**6. Success Metrics**\n",
    "- Clear clinical insights\n",
    "- Well-presented results\n",
    "- Team collaboration\n",
    "- Meeting deadlines\n",
    "- Quality of interpretation\n",
    "\n",
    "**7. Support Resources**\n",
    "- Template code provided\n",
    "- Basic statistical guidance\n",
    "- Visualization examples\n",
    "- Report templates\n",
    "- Regular check-ins\n",
    "\n",
    "**8. Final Deliverables Checklist**\n",
    "```markdown\n",
    "- [ ] Completed analysis\n",
    "- [ ] Generated figures\n",
    "- [ ] Written report\n",
    "- [ ] Presentation slides\n",
    "- [ ] Code documentation\n",
    "```\n",
    "\n",
    "**9. Learning Outcomes**\n",
    "1. Understanding PSN basics\n",
    "2. Basic data analysis skills\n",
    "3. Clinical data interpretation\n",
    "4. Scientific communication\n",
    "5. Team collaboration\n",
    "\n",
    "**10. Future Applications**\n",
    "- Foundation for advanced studies\n",
    "- Clinical research skills\n",
    "- Data analysis experience\n",
    "- Scientific writing practice\n",
    "\n",
    "**Remember:**\n",
    "- Focus on understanding over complexity\n",
    "- Prioritize clinical relevance\n",
    "- Work as a team\n",
    "- Use provided resources\n",
    "- Ask for help when needed\n",
    "\n",
    "This approach should help medical students:\n",
    "- Complete meaningful projects\n",
    "- Gain valuable skills\n",
    "- Stay within time constraints\n",
    "- Produce useful results\n",
    "- Have a positive learning experience\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elmed219-2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}